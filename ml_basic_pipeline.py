# -*- coding: utf-8 -*-
"""ML_basic_Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mh-0kmIh-9JOgAWIhIGiDaFYyMZOdhWf

# PREVENTING BANK CUSTOMER CHURN!

We aim to accomplist the following for this study:

Identify and visualize which factors contribute to customer churn:

Build a prediction model that will perform the following:

Classify if a customer is going to churn or not
Preferably and based on model performance, choose a model that will attach a probability to the churn to make it easier for customer service to target low hanging fruits in their efforts to prevent churn
"""



"""### Library importation"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from hyperopt import fmin, tpe, hp
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# For visualization
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
pd.options.display.max_rows = None
pd.options.display.max_columns = None

# 1. Reading data from CSV
def read_csv(file_path):
    return pd.read_csv(file_path)

# 2. Creating features
def create_features(data):
    # No feature creation for this example
    return data
# 3. Training a classifier model
def train_classifier(data):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = RandomForestClassifier()
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    return model, accuracy

# 4. Hyperparameter tuning with Hyperopt
def objective(params):
    model = RandomForestClassifier(**params)
    score = cross_val_score(model, X, y, cv=5).mean()
    return -score  # Minimize negative accuracy

# 5. Evaluating the model on the test set
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    return accuracy

file_path = '/content/drive/MyDrive/dataset/Bank_Churners.csv'

data = read_csv(file_path)
data.head()

data.info()

columns_to_drop = ['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',
                   'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2']
data = data.drop(columns=columns_to_drop,axis=1)

data.head()

data.isna().sum()

data['Education_Level'].value_counts()

data['Marital_Status'].value_counts()

#checking missing values
df_null_summary = pd.concat(
    [data.isnull().sum(), data.isnull().sum() *100/ data.isnull().count()], axis=1)

df_null_summary.columns = ["Null Record Count", "Percentage of Null Records"]
df_null_summary[df_null_summary["Null Record Count"] > 0].sort_values(
    by="Percentage of Null Records", ascending=False
).style.background_gradient(cmap="YlOrRd")

#get unique values of each colum
features = data.columns

for i in features:
    print('''Unique value of {}\n{}\nlen is {} \n##############\n
          '''.format(i, data[i].unique(),len(data[i].unique())))

# statistical summary of the numerical columns in the data
data.describe().T

# Statistical summary of the non-numerical columns in the data
data.describe(exclude = np.number).T

# printing unique value counts and percentages for the category/object type variables
def get_catagory_unique_value():
  for cat_cols in (data.select_dtypes(exclude=[np.int64, np.float64]).columns.unique().to_list()):
    print("Unique values and corresponding data counts for feature:" + cat_cols)
    print('-'*50)

    df_temp = pd.concat(
        [
            data[cat_cols].value_counts(),
            data[cat_cols].value_counts(normalize = True)
        ],
        axis=1
    )
    df_temp.columns = ["Count", "Percentage"]
    print(df_temp)
    print('-'*50)

get_catagory_unique_value()

# Dropping column - ID
data.drop(columns = ['CLIENTNUM'],axis = 1, inplace = True)

# subset to view incorrect values
data[data.Income_Category == "abc"]

len(data[data.Income_Category == "abc"])

data.Income_Category.value_counts()

# replace values with missing
data['Income_Category'].replace('abc', np.nan, inplace=True)

# subset to view incorrect values
data[data['Income_Category'] == 'abc']

data['Income_Category'].isna().sum()

data['Income_Category'] = data['Income_Category'].fillna(data['Income_Category'].value_counts().index[0])

# check value replacement
data['Income_Category'].value_counts()

data.isna().sum()

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from hyperopt import hp, tpe, fmin

# Split data into features and target
X = data.drop('Attrition_Flag', axis=1)
y = data['Attrition_Flag']

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify numeric and categorical features
numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X_train.select_dtypes(include=['object']).columns

# Define the space with hyperparameter choices
space = {
    'n_estimators': hp.choice('n_estimators', range(10, 101)),
    'max_depth': hp.choice('max_depth', range(1, 21))
}

# Preprocess the data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ],
    remainder='passthrough'
)

# Hyperparameter tuning using Tree of Parzen Estimators (TPE)
def objective(params):
    # Preprocess the data
    X_train_preprocessed = preprocessor.fit_transform(X_train)

    model = RandomForestClassifier(**params)
    score = cross_val_score(model, X_train_preprocessed, y_train, cv=5).mean()
    return -score  # Minimize negative accuracy

best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100)

# Use best hyperparameters to create a new pipeline
best_params_eval = space_eval(space, best_params)

# Preprocess the entire training set with the best hyperparameters
X_train_preprocessed = preprocessor.fit_transform(X_train)

pipeline_hyp = Pipeline([
    ('classifier', RandomForestClassifier(**best_params_eval))
])

# Train the model with the best hyperparameters
pipeline_hyp.fit(X_train_preprocessed, y_train)

# Preprocess the test set with the best hyperparameters
X_test_preprocessed = preprocessor.transform(X_test)

# Evaluate the model
y_pred_hyp = pipeline_hyp.predict(X_test_preprocessed)
accuracy_hyp = accuracy_score(y_test, y_pred_hyp)
print(f"Model accuracy after hyperparameter tuning on the test set: {accuracy_hyp}")